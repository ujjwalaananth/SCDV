{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "from numpy import float32\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pickle\n",
    "#import cPickle\n",
    "from math import *\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def drange(start, stop, step):\n",
    "\tr = start\n",
    "\twhile r < stop:\n",
    "\t\tyield r\n",
    "\t\tr += step\n",
    "\n",
    "def cluster_GMM(num_clusters, word_vectors):\n",
    "\t# Initalize a GMM object and use it for clustering.\n",
    "\tclf =  GaussianMixture(n_components=num_clusters,\n",
    "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "\t# Get cluster assignments.\n",
    "\tclf.fit(word_vectors)\n",
    "\tidx = clf.predict(word_vectors)\n",
    "\tprint (\"Clustering Done...\", time.time()-start, \"seconds\")\n",
    "\t# Get probabilities of cluster assignments.\n",
    "\tidx_proba = clf.predict_proba(word_vectors)\n",
    "\tprint ('prob vector size' + str(idx_proba.shape))\n",
    "\tprint (pd.DataFrame(idx_proba))\n",
    "\t# Dump cluster assignments and probability of cluster assignments. \n",
    "\tjoblib.dump(idx, 'gmm_latestclusmodel_len2alldata.pkl')\n",
    "\tprint (\"Cluster Assignments Saved...\")\n",
    "\t\n",
    "\tjoblib.dump(idx_proba, 'gmm_prob_latestclusmodel_len2alldata.pkl')\n",
    "\tprint (\"Probabilities of Cluster Assignments Saved...\")\n",
    "\treturn (idx, idx_proba)\n",
    "\n",
    "def read_GMM(idx_name, idx_proba_name):\n",
    "\t# Loads cluster assignments and probability of cluster assignments. \n",
    "\tidx = joblib.load(idx_name)\n",
    "\tidx_proba = joblib.load(idx_proba_name)\n",
    "\tprint (\"Cluster Model Loaded...\")\n",
    "\treturn (idx, idx_proba)\n",
    "\n",
    "def get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict):\n",
    "\t# This function computes probability word-cluster vectors.\n",
    "\t\n",
    "\tprob_wordvecs = {}\n",
    "\n",
    "\tfor word in word_centroid_map:\n",
    "\t\tprob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "\t\tfor index in range(0, num_clusters):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprob_wordvecs[word][index*num_features:(index+1)*num_features] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t# prob_wordvecs_idf_len2alldata = {}\n",
    "\t# i = 0\n",
    "\t# for word in featurenames:\n",
    "\t# \ti += 1\n",
    "\t# \tif word in word_centroid_map:\t\n",
    "\t# \t\tprob_wordvecs_idf_len2alldata[word] = {}\n",
    "\t# \t\tfor index in range(0, num_clusters):\n",
    "\t# \t\t\t\tprob_wordvecs_idf_len2alldata[word][index] = model[word] * word_centroid_prob_map[word][index] * word_idf_dict[word] \n",
    "\n",
    "\t\n",
    "\n",
    "\t# for word in prob_wordvecs_idf_len2alldata.keys():\n",
    "\t# \tprob_wordvecs[word] = prob_wordvecs_idf_len2alldata[word][0]\n",
    "\t# \tfor index in prob_wordvecs_idf_len2alldata[word].keys():\n",
    "\t# \t\tif index==0:\n",
    "\t# \t\t\tcontinue\n",
    "\t# \t\tprob_wordvecs[word] = np.concatenate((prob_wordvecs[word], prob_wordvecs_idf_len2alldata[word][index]))\n",
    "#\tprint(\"Each doc shape is:\"); print (prob_wordvecs[0].shape)\n",
    "\treturn prob_wordvecs\n",
    "\n",
    "def create_cluster_vector_and_gwbowv(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "\t# This function computes SDV feature vectors.\n",
    "\tbag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "\tglobal min_no\n",
    "\tglobal max_no\n",
    "\n",
    "\tfor word in wordlist:\n",
    "\t\ttry:\n",
    "\t\t\ttemp = word_centroid_map[word]\n",
    "\t\texcept:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tbag_of_centroids += prob_wordvecs[word]\n",
    "\n",
    "\tnorm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "\tif(norm!=0):\n",
    "\t\tbag_of_centroids /= norm\n",
    "\n",
    "\t# To make feature vector sparse, make note of minimum and maximum values.\n",
    "\tif train:\n",
    "\t\tmin_no += min(bag_of_centroids)\n",
    "\t\tmax_no += max(bag_of_centroids)\n",
    "\n",
    "\treturn bag_of_centroids\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tstart = time.time()\n",
    "\n",
    "\tnum_features = 200 #int(sys.argv[1])     # Word vector dimensionality\n",
    "\tmin_word_count = 20   # Minimum word count\n",
    "\tnum_workers = 40       # Number of threads to run in parallel\n",
    "\tcontext = 10          # Context window size\n",
    "\tdownsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\tmodel_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "  \t# Load the trained Word2Vec model.\n",
    "\tmodel = Word2Vec.load(model_name)\n",
    "  \t# Get wordvectors for all words in vocabulary.\n",
    "\tword_vectors = model.wv.vectors #older ver: word_vectors = model.wv.syn0\n",
    "\n",
    "\t# Load train data.\n",
    "\ttrain = pd.read_csv( 'data/train_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "\t# Load test data.\n",
    "\ttest = pd.read_csv( 'data/test_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "\tall = pd.read_csv( 'data/all_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "\n",
    "\t# Set number of clusters.\n",
    "\tnum_clusters = 60 #int(sys.argv[2])\n",
    "\t# Uncomment below line for creating new clusters.\n",
    "\tprint ('word vector size:'); print (word_vectors.shape)\n",
    "\tidx, idx_proba = cluster_GMM(num_clusters, word_vectors)\n",
    "\n",
    "\t# Uncomment below lines for loading saved cluster assignments and probability of cluster assignments.\n",
    "\t# idx_name = \"gmm_latestclusmodel_len2alldata.pkl\"\n",
    "\t# idx_proba_name = \"gmm_prob_latestclusmodel_len2alldata.pkl\"\n",
    "\t# idx, idx_proba = read_GMM(idx_name, idx_proba_name)\n",
    "\n",
    "\t# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "\t# a cluster number\n",
    "\tword_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "\t# Create a Word / Probability of cluster assignment dictionary, mapping each vocabulary word to\n",
    "\t# list of probabilities of cluster assignments.\n",
    "\tword_centroid_prob_map = dict(zip( model.wv.index2word, idx_proba ))\n",
    "\n",
    "\t# Computing tf-idf values.\n",
    "\ttraindata = []\n",
    "\tfor i in range( 0, len(all[\"news\"])):\n",
    "\t\ttraindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(all[\"news\"][i], True)))\n",
    "\n",
    "\ttfv = TfidfVectorizer(strip_accents='unicode',dtype=np.float32)\n",
    "\ttfidfmatrix_traindata = tfv.fit_transform(traindata)\n",
    "\tfeaturenames = tfv.get_feature_names()\n",
    "\tidf = tfv._tfidf.idf_\n",
    "\n",
    "\t# Creating a dictionary with word mapped to its idf value \n",
    "\tprint (\"Creating word-idf dictionary for Training set...\")\n",
    "\n",
    "\tword_idf_dict = {}\n",
    "\tfor pair in zip(featurenames, idf):\n",
    "\t\tword_idf_dict[pair[0]] = pair[1]\n",
    "\n",
    "\t# Pre-computing probability word-cluster vectors.\n",
    "\tprob_wordvecs = get_probability_word_vectors(featurenames, word_centroid_map, num_clusters, word_idf_dict)\n",
    "\ttemp_time = time.time() - start\n",
    "\tprint (\"Creating Document Vectors...:\", temp_time, \"seconds.\")\n",
    "\tprint (\"Reducing dim of Document Vectors...:\", temp_time, \"seconds.\")\n",
    "\n",
    "\n",
    "\t# gwbowv is a matrix which contains normalised document vectors.\n",
    "\tgwbowv = np.zeros( (train[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")\n",
    "\tmin_no = 0\n",
    "\tmax_no = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 15591)\n",
      "Train News Covered :  1000\n",
      "doc vec shape :  (12000,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f390e5063769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Get the wordlist in each news article.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleWord2VecUtility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mgwbowv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cluster_vector_and_gwbowv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_wordvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_centroid_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_centroid_prob_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idf_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-9213b8936fe3>\u001b[0m in \u001b[0;36mcreate_cluster_vector_and_gwbowv\u001b[0;34m(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train)\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mbag_of_centroids\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprob_wordvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'...i,...i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_of_centroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag_of_centroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\tpca=PCA(n_components=4) #for dim reduction into subspaces\n",
    "\tcounter = 0; \n",
    "\tfor review in train[\"news\"]:\n",
    "\t\t# Get the wordlist in each news article.\n",
    "\t\twords = KaggleWord2VecUtility.review_to_wordlist( review, \\\n",
    "            remove_stopwords=True )\n",
    "\t\tgwbowv[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters, train=True)\n",
    "\t\tcounter+=1\n",
    "\t\tif counter % 1000 == 0:\n",
    "\t\t\tprint (\"Train News Covered : \",counter)\n",
    "\t\t\tprint (\"doc vec shape : \",gwbowv[counter].shape)\n",
    "\n",
    "\tgwbowv_name = \"SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n",
    "\tprint(gwbowv.shape)\n",
    "\n",
    "\n",
    "\tgwbowv_test = np.zeros( (test[\"news\"].size, num_clusters*(num_features)), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'From: adamsj@gtewd.mtv.gtegsc.com Subject: Re: Homosexuality issues in Christianity Reply To: adamsj@gtewd.mtv.gtegsc.com Organization: GTE Govt. Systems, Electronics Def. Div. Lines: 18  In article  May.13.02.29.39.1993.1505@geneva.rutgers.edu , revdak@netcom.com (D. Andrew Kille) writes:   Of course the whole issue is one of discernment.  It may be that Satan   is trying to convince us that we know more than God.  Or it may be that   God is trying (as God did with Peter) to teach us something we don\\'t   know  that \"God shows no partiality, but in every nation anyone who fears   him and does what is right is acceptable to him.\" (Acts 10:34 35).      revdak@netcom.com  Fine, but one of the points of this entire discussion is that \"we\" (conservative, reformed christians   this could start an argument... But isn\\'t this idea that homosexuality is ok fairly \"new\"  this century  ? Is there any support for this being a viable viewpoint before this century? I don\\'t know.) don\\'t believe that homosexuality is \"acceptable to Him\". So your scripture quotation doesn\\'t work for \"us\".   jeff adams  '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b6e72b4e2043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"news\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"news\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"news\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Get the wordlist in each news article.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleWord2VecUtility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[0;32m--> 370\u001b[0;31m                         copy=self.copy)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# Handle n_components==None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'From: adamsj@gtewd.mtv.gtegsc.com Subject: Re: Homosexuality issues in Christianity Reply To: adamsj@gtewd.mtv.gtegsc.com Organization: GTE Govt. Systems, Electronics Def. Div. Lines: 18  In article  May.13.02.29.39.1993.1505@geneva.rutgers.edu , revdak@netcom.com (D. Andrew Kille) writes:   Of course the whole issue is one of discernment.  It may be that Satan   is trying to convince us that we know more than God.  Or it may be that   God is trying (as God did with Peter) to teach us something we don\\'t   know  that \"God shows no partiality, but in every nation anyone who fears   him and does what is right is acceptable to him.\" (Acts 10:34 35).      revdak@netcom.com  Fine, but one of the points of this entire discussion is that \"we\" (conservative, reformed christians   this could start an argument... But isn\\'t this idea that homosexuality is ok fairly \"new\"  this century  ? Is there any support for this being a viable viewpoint before this century? I don\\'t know.) don\\'t believe that homosexuality is \"acceptable to Him\". So your scripture quotation doesn\\'t work for \"us\".   jeff adams  '"
     ]
    }
   ],
   "source": [
    "\tcounter = 0\n",
    "\tfor review in test[\"news\"]:\n",
    "\t\t# Get the wordlist in each news article.\n",
    "\t\twords = KaggleWord2VecUtility.review_to_wordlist( review, \\\n",
    "            remove_stopwords=True )\n",
    "\t\tgwbowv_test[counter] = create_cluster_vector_and_gwbowv(prob_wordvecs, words, word_centroid_map, word_centroid_prob_map, num_features, word_idf_dict, featurenames, num_clusters)\n",
    "\t\tcounter+=1\n",
    "\t\tif counter % 1000 == 0:\n",
    "\t\t\tprint (\"Test News Covered : \",counter)\n",
    "\n",
    "    \n",
    "\ttest_gwbowv_name = \"TEST_SDV_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_gmm_sparse.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\tprint (\"Making sparse...\")\n",
    "\t# Set the threshold percentage for making it sparse. \n",
    "\tpercentage = 0.04\n",
    "\tmin_no = min_no*1.0/len(train[\"news\"])\n",
    "\tmax_no = max_no*1.0/len(train[\"news\"])\n",
    "\tprint (\"Average min: \", min_no)\n",
    "\tprint (\"Average max: \", max_no)\n",
    "\tthres = (abs(max_no) + abs(min_no))/2\n",
    "\tthres = thres*percentage\n",
    "\n",
    "\t# Make values of matrices which are less than threshold to zero.\n",
    "\ttemp = abs(gwbowv) < thres\n",
    "\tgwbowv[temp] = 0\n",
    "\n",
    "\ttemp = abs(gwbowv_test) < thres\n",
    "\tgwbowv_test[temp] = 0\n",
    "\t\n",
    "\t#saving gwbowv train and test matrices\n",
    "\tnp.save(gwbowv_name, gwbowv)\n",
    "\tnp.save(test_gwbowv_name, gwbowv_test)\n",
    "\t\n",
    "\tendtime = time.time() - start\n",
    "\tprint (\"SDV created and dumped: \", endtime, \"seconds.\")\n",
    "\tprint (\"Fitting a SVM classifier on labeled training data...\")\n",
    "\n",
    "\tparam_grid = [\n",
    "\t  {'C': np.arange(0.1, 7, 0.2)}]\n",
    "\tscores = ['f1_weighted']#,'accuracy', 'recall_micro', 'f1_micro' , 'precision_micro', 'recall_macro', 'f1_macro' , 'precision_macro', 'recall_weighted', 'precision_weighted','f1_weighted'] #, 'accuracy', 'recall', 'f1']\n",
    "\tfor score in scores:\n",
    "\t\tstrt = time.time()\n",
    "\t\tprint (\"# Tuning hyper-parameters for\", score, \"\\n\")\n",
    "\t\tclf = GridSearchCV(LinearSVC(C=1), param_grid, cv=5, scoring= '%s' % score)\n",
    "\t\tclf.fit(gwbowv, train[\"class\"])\n",
    "\t\tprint (\"Best parameters set found on development set:\\n\")\n",
    "\t\tprint (clf.best_params_)\n",
    "\t\tprint (\"Best value for \", score, \":\\n\")\n",
    "\t\tprint (clf.best_score_)\n",
    "\t\tY_true, Y_pred  = test[\"class\"], clf.predict(gwbowv_test)\n",
    "\t\tprint (\"Report\")\n",
    "\t\tprint (classification_report(Y_true, Y_pred, digits=6))\n",
    "\t\tprint (\"Accuracy: \",clf.score(gwbowv_test,test[\"class\"]))\n",
    "\t\tprint (\"Time taken:\", time.time() - strt, \"\\n\")\n",
    "\tendtime = time.time()\n",
    "\tprint (\"Total time taken: \", endtime-start, \"seconds.\" )\n",
    "\n",
    "\tprint (\"********************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
