{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached https://files.pythonhosted.org/packages/86/f3/37504f07651330ddfdefa631ca5246974a60d0908216539efda842fd080f/gensim-3.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting scipy>=0.18.1 (from gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "Collecting numpy>=1.11.3 (from gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/88/29/f4c845648ed23264e986cdc5fbab5f8eace1be5e62144ef69ccc7189461d/numpy-1.15.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting six>=1.5.0 (from gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/29/ad/b7775a16329acfbf6eecf0198c05caa8a067c518fd84728b3aaad60446ea/boto3-1.7.65-py2.py3-none-any.whl\n",
      "Collecting requests (from smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl\n",
      "Collecting botocore<1.11.0,>=1.10.65 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/08/08/c2c28d9d4d81e817aa4c62ae580d3e2d091c7f21345de3494641f78408b2/botocore-1.10.65-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/7c/e6/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5/certifi-2018.4.16-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<2.8,>=2.5 (from requests->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.24,>=1.21.1 (from requests->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" (from botocore<1.11.0,>=1.10.65->boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/cf/f5/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825/python_dateutil-2.7.3-py2.py3-none-any.whl\n",
      "Collecting docutils>=0.10 (from botocore<1.11.0,>=1.10.65->boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl\n",
      "Installing collected packages: numpy, scipy, bz2file, boto, six, python-dateutil, jmespath, docutils, botocore, s3transfer, boto3, certifi, chardet, idna, urllib3, requests, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.7.65 botocore-1.10.65 bz2file-0.98 certifi-2018.4.16 chardet-3.0.4 docutils-0.14 gensim-3.5.0 idna-2.7 jmespath-0.9.3 numpy-1.15.0 python-dateutil-2.7.3 requests-2.19.1 s3transfer-0.1.13 scipy-1.1.0 six-1.11.0 smart-open-1.6.0 urllib3-1.23\n",
      "Collecting nltk\n",
      "Collecting six (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, nltk\n",
      "Successfully installed nltk-3.3 six-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "from numpy import float32\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pickle\n",
    "from math import *\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(word_vectors,test_word_vectors,num_clusters):\n",
    "    clf=KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    clf.fit(word_vectors)\n",
    "    idx=clf.predict(word_vectors)\n",
    "    idx_test=clf.predict(test_word_vectors)\n",
    "    #idx_proba=clf.predict_proba(word_vectors)\n",
    "    centroids=clf.cluster_centers_\n",
    "    return (idx,idx_test,centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdocvec(review,model,features):\n",
    "    wordlist=review.split()\n",
    "    #print (len(wordlist))\n",
    "    docvec=np.zeros([len(wordlist),features], dtype=\"float32\")\n",
    "    i=0\n",
    "    for word in wordlist:\n",
    "        if(word in model.wv.vocab):\n",
    "            #print (word)\n",
    "            docvec[i]=model[word]\n",
    "            i+=1\n",
    "    #print (pca)\n",
    "    #docvec=(pca.fit_transform(docvec.transpose())).transpose()\n",
    "    docvec=np.asmatrix(docvec)\n",
    "    #print (docvec.shape)\n",
    "    return docvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 200 #int(sys.argv[1])     # Word vector dimensionality\n",
    "min_word_count = 20   # Minimum word count\n",
    "num_workers = 40       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "sub_dim=3   #dimension for subspaces\n",
    "model_name = str(num_features) + \"features_\" + str(min_word_count) + \"minwords_\" + str(context) + \"context_len2alldata\"\n",
    "# Load the trained Word2Vec model. \n",
    "model = Word2Vec.load(model_name)\n",
    "# Get wordvectors for all words in vocabulary.\n",
    "word_vectors = model.wv.vectors\n",
    "# Load train data.\n",
    "train = pd.read_csv( 'data/train_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "# Load test data.\n",
    "test = pd.read_csv( 'data/test_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "all = pd.read_csv( 'data/all_v2.tsv', header=0, delimiter=\"\\t\")\n",
    "# Set number of clusters.\n",
    "num_clusters = 40 #int(sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lerxst wam umd edu thing subject car nntp posting host rac wam umd edu organization university maryland college park lines wondering anyone could enlighten car saw day door sports car looked late early called bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car made history whatever info funky looking car please mail thanks il brought neighborhood lerxst\n",
      "(18846, 2)\n"
     ]
    }
   ],
   "source": [
    "traindata = []\n",
    "for i in range( 0, len(all[\"news\"])):\n",
    "\ttraindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(all[\"news\"][i], True)))\n",
    "print (traindata[0])\n",
    "print (all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word-idf dictionary for Training set...\n"
     ]
    }
   ],
   "source": [
    "tfv = TfidfVectorizer(strip_accents='unicode',dtype=np.float32)\n",
    "tfidfmatrix_traindata = tfv.fit_transform(traindata)\n",
    "featurenames = tfv.get_feature_names()\n",
    "idf = tfv._tfidf.idf_\n",
    "# Creating a dictionary with word mapped to its idf value \n",
    "print (\"Creating word-idf dictionary for Training set...\")\n",
    "\n",
    "word_idf_dict = {}\n",
    "for pair in zip(featurenames, idf):\n",
    "\tword_idf_dict[pair[0]] = pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 2) (11314, 3) (7532, 3)\n"
     ]
    }
   ],
   "source": [
    "print (all.shape,train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews covered: 0\n",
      "(3, 200)\n",
      "Train reviews covered: 1000\n",
      "(3, 200)\n",
      "Train reviews covered: 2000\n",
      "(3, 200)\n",
      "Train reviews covered: 3000\n",
      "(3, 200)\n",
      "Train reviews covered: 4000\n",
      "(3, 200)\n",
      "Train reviews covered: 5000\n",
      "(3, 200)\n",
      "Train reviews covered: 6000\n",
      "(3, 200)\n",
      "Train reviews covered: 7000\n",
      "(3, 200)\n",
      "Train reviews covered: 8000\n",
      "(3, 200)\n",
      "Train reviews covered: 9000\n",
      "(3, 200)\n",
      "Train reviews covered: 10000\n",
      "(3, 200)\n",
      "Train reviews covered: 11000\n",
      "(3, 200)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#converting training data into 4x200 subspaces\n",
    "doc4vecs=np.zeros((len(train[\"news\"]),sub_dim,num_features))\n",
    "doccovrep=np.zeros((len(train[\"news\"]),num_features,num_features))\n",
    "pca=PCA(sub_dim) #pca object for dimensionality reduction\n",
    "docvec=[]\n",
    "for i in range(0,len(train[\"news\"])):\n",
    "    docvec=getdocvec(train[\"news\"][i],model,num_features)\n",
    "    #converting to 4x200 subspace\n",
    "    doc4vecs[i]=(pca.fit_transform(docvec.transpose())).transpose()\n",
    "    #doccovrep[i]=np.cov(doc4vecs[i].transpose())\n",
    "    #doc4vecs[i]=docvec\n",
    "    if(i%1000==0):\n",
    "        print (\"Train reviews covered:\",i)\n",
    "        print (doc4vecs[i].shape)\n",
    "        #print (doccovrep[i].shape)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test reviews covered: 0\n",
      "(3, 200)\n",
      "Test reviews covered: 1000\n",
      "(3, 200)\n",
      "Test reviews covered: 2000\n",
      "(3, 200)\n",
      "Test reviews covered: 3000\n",
      "(3, 200)\n",
      "Test reviews covered: 4000\n",
      "(3, 200)\n",
      "Test reviews covered: 5000\n",
      "(3, 200)\n",
      "Test reviews covered: 6000\n",
      "(3, 200)\n",
      "Test reviews covered: 7000\n",
      "(3, 200)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#converting test data into 4x200 subspaces,idx_proba.shape\n",
    "doc4vecs_test=np.zeros((len(test[\"news\"]),sub_dim,num_features))\n",
    "doccovrep_test=np.zeros((len(test[\"news\"]),num_features,num_features))\n",
    "\n",
    "docvec=[]\n",
    "for i in range(0,len(test[\"news\"])):\n",
    "    docvec=getdocvec(test[\"news\"][i],model,num_features)\n",
    "    #converting to 4x200 subspace\n",
    "    doc4vecs_test[i]=(pca.fit_transform(docvec.transpose())).transpose()\n",
    "    #doccovrep_test[i]=np.cov(doc4vecs_test[i].transpose())\n",
    "\n",
    "    #doc4vecs_test[i]=docvec\n",
    "    if(i%1000==0):\n",
    "        print (\"Test reviews covered:\",i)\n",
    "        print (doc4vecs_test[i].shape)\n",
    "        #print (doccovrep[i].shape)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTING COVARIANCE METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting 4x4 or 3x3 cov matrix for each subspace, therefore for each document.\n",
    "Then we flatten it to dim 10 or 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subspaces made mean:  0\n",
      "(200,)\n",
      "Training subspaces made mean:  1000\n",
      "(200,)\n",
      "Training subspaces made mean:  2000\n",
      "(200,)\n",
      "Training subspaces made mean:  3000\n",
      "(200,)\n",
      "Training subspaces made mean:  4000\n",
      "(200,)\n",
      "Training subspaces made mean:  5000\n",
      "(200,)\n",
      "Training subspaces made mean:  6000\n",
      "(200,)\n",
      "Training subspaces made mean:  7000\n",
      "(200,)\n",
      "Training subspaces made mean:  8000\n",
      "(200,)\n",
      "Training subspaces made mean:  9000\n",
      "(200,)\n",
      "Training subspaces made mean:  10000\n",
      "(200,)\n",
      "Training subspaces made mean:  11000\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "#getting mean vector for training subspaces\n",
    "mean_doc_vecs=np.zeros((len(doc4vecs),num_features))\n",
    "for i in range(0,len(doc4vecs)):\n",
    "    for j in range(0,sub_dim):\n",
    "        mean_doc_vecs[i]+=doc4vecs[i][j] #add 200d of each dimension of subspace\n",
    "        j+=1\n",
    "    mean_doc_vecs/=sub_dim #averaging\n",
    "    if(i%1000==0):\n",
    "        print(\"Training subspaces made mean: \",i)\n",
    "        print(mean_doc_vecs[i].shape)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20100\n",
      "0\n",
      "(20100,)\n",
      "1000\n",
      "(20100,)\n",
      "2000\n",
      "(20100,)\n",
      "3000\n",
      "(20100,)\n",
      "4000\n",
      "(20100,)\n",
      "5000\n",
      "(20100,)\n",
      "6000\n",
      "(20100,)\n",
      "7000\n",
      "(20100,)\n",
      "8000\n",
      "(20100,)\n",
      "9000\n",
      "(20100,)\n",
      "10000\n",
      "(20100,)\n",
      "11000\n",
      "(20100,)\n",
      "(11314, 20100)\n"
     ]
    }
   ],
   "source": [
    "#covariance matrix + flattening for training subspaces\n",
    "cov_dim=int((num_features*(num_features+1))/2)\n",
    "print (cov_dim)\n",
    "covrep=np.zeros((len(doc4vecs),cov_dim))\n",
    "for i in range(0,len(doc4vecs)):\n",
    "    #finding covariance matrix\n",
    "    cov=np.cov(doc4vecs[i].transpose())\n",
    "    #flattening\n",
    "    a=0\n",
    "    for p in range(0,num_features):\n",
    "        for q in range(p,num_features):\n",
    "            if(p==q):\n",
    "                covrep[i][a]=cov[p][q]\n",
    "            else:\n",
    "                covrep[i][a]=cov[p][q]*np.sqrt(2)\n",
    "            a+=1\n",
    "            q+=1\n",
    "        p+=1\n",
    "    if(i%1000==0):\n",
    "        print (i)\n",
    "        print (covrep[i].shape)\n",
    "    i+=1\n",
    "    \n",
    "print (covrep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting mean vector for test subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20100\n",
      "0\n",
      "(20100,)\n",
      "1000\n",
      "(20100,)\n",
      "2000\n",
      "(20100,)\n",
      "3000\n",
      "(20100,)\n",
      "4000\n",
      "(20100,)\n",
      "5000\n",
      "(20100,)\n",
      "6000\n",
      "(20100,)\n",
      "7000\n",
      "(20100,)\n",
      "(7532, 20100)\n"
     ]
    }
   ],
   "source": [
    "#covariance matrix + flattening for test subspaces\n",
    "print (cov_dim)\n",
    "covrep_test=np.zeros((len(doc4vecs_test),cov_dim))\n",
    "for i in range(0,len(doc4vecs_test)):\n",
    "    #finding covariance matrix\n",
    "    cov=np.cov(doc4vecs_test[i].transpose())\n",
    "    #flattening\n",
    "    a=0\n",
    "    for p in range(0,num_features):\n",
    "        for q in range(p,num_features):\n",
    "            if(p==q):\n",
    "                covrep_test[i][a]=cov[p][q]\n",
    "            else:\n",
    "                covrep_test[i][a]=cov[p][q]*np.sqrt(2)\n",
    "            a+=1\n",
    "            q+=1\n",
    "        p+=1\n",
    "    if(i%1000==0):\n",
    "        print (i)\n",
    "        print (covrep_test[i].shape)\n",
    "    i+=1\n",
    "    \n",
    "print (covrep_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print (scipy.sparse.issparse(covrep_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering word vectors\n",
    "#trainingvecs=np.zeros((sub_dim*len(doc4vecs),num_features))\n",
    "trainingvecs=np.zeros((num_features*len(doc4vecs),num_features)) #for cov rep\n",
    "j=0\n",
    "k=0\n",
    "for i in range(0,num_features*len(doc4vecs)):\n",
    "    trainingvecs[i]=doccovrep[j][k]\n",
    "    k+=1 #move on to next set of 200 features\n",
    "    k=k%num_features #%sub_dim\n",
    "    i+=1 #next 1x200 vector sample\n",
    "    if(k==0):\n",
    "        j+=1 #move on to next doc\n",
    "    if(i%30000==0):\n",
    "        print (\"Training vectors gathered: \",i)\n",
    "print (trainingvecs.shape)\n",
    "#print (doc4vecs[1][2][0:3])\n",
    "#print (trainingvecs[6][0:3]) #confirming that vectors were stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering word vectors\n",
    "#testvecs=np.zeros((sub_dim*len(doc4vecs_test),num_features))\n",
    "testvecs=np.zeros((num_features*len(doc4vecs_test),num_features))\n",
    "j=0\n",
    "k=0\n",
    "for i in range(0,num_features*len(doc4vecs_test)):\n",
    "    testvecs[i]=doccovrep_test[j][k]\n",
    "    k+=1 #move on to next set of 200 features\n",
    "    k=k%num_features #%sub_dim\n",
    "    i+=1 #next 1x200 vector sample\n",
    "    if(k==0):\n",
    "        j+=1 #move on to next set of doc\n",
    "    if(i%3000==0):\n",
    "        print (\"Test vectors gathered: \",i)\n",
    "print (testvecs.shape)\n",
    "#print (doc4vecs_test[1][2][0:3])\n",
    "#print (testvecs[6][0:3]) #confirming that vectors were stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering these vectors\n",
    "idx,idx_test,centroids=clustering(trainingvecs,testvecs,num_clusters)\n",
    "print (idx.shape, centroids.shape)\n",
    "print (Counter(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIRECTLY CONCATENATING 4x200 VECTORS\n",
    "train_con4features=np.zeros((len(train),sub_dim*num_features))\n",
    "for i in range(0,len(doc4vecs)):\n",
    "    for j in range(0,4):\n",
    "        for k in range(0,num_features):\n",
    "            train_con4features[i][(j*num_features)+k]=doc4vecs[i][j][k]\n",
    "            k+=1\n",
    "        j+=1\n",
    "    i+=1\n",
    "print (train_con4features.shape)\n",
    "#for test dataset\n",
    "test_con4features=np.zeros((len(test),sub_dim*num_features))\n",
    "for i in range(0,len(doc4vecs_test)):\n",
    "    for j in range(0,4):\n",
    "        for k in range(0,num_features):\n",
    "            test_con4features[i][(j*num_features)+k]=doc4vecs_test[i][j][k]\n",
    "            k+=1\n",
    "        j+=1\n",
    "    i+=1\n",
    "print (test_con4features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT DIRECTLY CONCATENATING, BUT AVERAGING VECTORS [not averaging, just adding]\n",
    "#SO EACH DOCUMENT HAS 1X200 REP, AVERAGE OF ITS 4\n",
    "train_avg4features=np.zeros((len(train),num_features))\n",
    "for i in range(0,len(doc4vecs)):\n",
    "    for j in range(0,4):\n",
    "        train_avg4features[i]+=doc4vecs[i][j] #adding all 4 vectors of doc number i\n",
    "        j+=1\n",
    "    #train_avg4features/=4 #averaging\n",
    "    if (i%3000==0):\n",
    "        print (train_avg4features[i][0:2])  \n",
    "    i+=1\n",
    "print (train_avg4features.shape)\n",
    "\n",
    "test_avg4features=np.zeros((len(test),num_features))\n",
    "for i in range(0,len(doc4vecs_test)):\n",
    "    for j in range(0,4):\n",
    "        if (i%3000==0):\n",
    "            print (test_avg4features[i][0:2],doc4vecs_test[i][j][0:2])\n",
    "            print (test_avg4features[i][0:2]+doc4vecs_test[i][j][0:2])\n",
    "        test_avg4features[i]+=doc4vecs_test[i][j] #adding all 4 vectors of doc number i\n",
    "        j+=1\n",
    "    #test_avg4features/=4 #averaging\n",
    "    if (i%3000==0):\n",
    "        print (test_avg4features[i][0:2])  \n",
    "    i+=1\n",
    "print (test_avg4features.shape)\n",
    "print (test_avg4features[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating training input vector\n",
    "train_mat=np.zeros((len(doc4vecs),num_clusters*num_features))\n",
    "print (train_mat.shape)\n",
    "j=0\n",
    "k=0\n",
    "#trainingvecs[0][0]\n",
    "#print (train_mat[0])\n",
    "for i in range(0,len(trainingvecs)):\n",
    "    #print (i,j,idx[i])\n",
    "    vec_clus=int(idx[i])\n",
    "    for k in range(0,num_features):\n",
    "    #print (idx[i])\n",
    "        train_mat[j][(num_features*vec_clus)+k] += trainingvecs[i][k]\n",
    "        #print (train_mat[j][(200*vec_clus)+k],trainingvecs[i][k])\n",
    "    i+=1\n",
    "    j=int(i/sub_dim)\n",
    "    \n",
    "print (idx[0:4])\n",
    "#print (trainingvecs[0])\n",
    "#print (train_mat[0][8000:8200]==trainingvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test input vector\n",
    "j=0\n",
    "k=0\n",
    "test_mat=np.zeros((len(doc4vecs_test),num_clusters*num_features))\n",
    "print (test_mat.shape)    \n",
    "for i in range(0,len(testvecs)):\n",
    "    #print (i,j,idx[i])\n",
    "    vec_clus=int(idx_test[i])\n",
    "    for k in range(0,num_features):\n",
    "    #print (idx[i])\n",
    "        test_mat[j][(num_features*vec_clus)+k] += testvecs[i][k]\n",
    "        #print (train_mat[j][(200*vec_clus)+k],trainingvecs[i][k])\n",
    "    i+=1\n",
    "    j=int(i/sub_dim)\n",
    "print (idx_test[0:4])\n",
    "#print (test_mat[0][200:400]==testvecs[0])\n",
    "#print (train[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6bdc0659eb72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"TEST_SUB_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cluster_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"feature_matrix_kmeans_sparse.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SUB_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cluster_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"feature_matrix_kmeans_sparse.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_mat' is not defined"
     ]
    }
   ],
   "source": [
    "#saving feature matrices\n",
    "test_name = \"TEST_SUB_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_kmeans_sparse.npy\"\n",
    "train_name = \"SUB_\" + str(num_clusters) + \"cluster_\" + str(num_features) + \"feature_matrix_kmeans_sparse.npy\"\n",
    "np.save(train_name, train_mat)\n",
    "np.save(test_name, test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(covrep, train[\"class\"])\n",
    "Y_true, Y_pred  = test[\"class\"], clf.predict(covrep_test)\n",
    "print (classification_report(Y_true, Y_pred, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1_weighted \n",
      " Running with  40  clusters\n"
     ]
    }
   ],
   "source": [
    "#applying classifier\n",
    "start=time.time()\n",
    "param_grid = [{'C': np.arange(0.1, 4.0, 0.6)}]\n",
    "scores = ['f1_weighted']#,'accuracy', 'recall_micro', 'f1_micro' , 'precision_micro', 'recall_macro', 'f1_macro' , 'precision_macro', 'recall_weighted', 'precision_weighted','f1_weighted'] #, 'accuracy', 'recall', 'f1']\n",
    "for score in scores:\n",
    "\tstrt = time.time()\n",
    "\tprint (\"# Tuning hyper-parameters for\", score, \"\\n\", \"Running with \",num_clusters,\" clusters\")\n",
    "\tclf = GridSearchCV(LinearSVC(C=1), param_grid, cv=3, scoring= '%s' % score)\n",
    "\t#clf = LinearSVC()\n",
    "\tclf.fit(covrep, train[\"class\"])\n",
    "\tendtime = time.time()\n",
    "\tprint (\"Best parameters set found on development set:\\n\")\n",
    "\tprint (clf.best_params_)\n",
    "\tprint (\"Best value for \", score, \":\\n\")\n",
    "\tprint (clf.best_score_)\n",
    "\tY_true, Y_pred  = test[\"class\"], clf.predict(covrep_test)\n",
    "\tprint (\"Report\")\n",
    "\tprint (classification_report(Y_true, Y_pred, digits=6))\n",
    "\tprint (\"Accuracy: \",clf.score(covrep_test,test[\"class\"]))\n",
    "\tprint (\"Time taken:\", endtime - start, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (doc4vecs.shape)\n",
    "print (trainingvecs.shape)\n",
    "print (train_avg4features.shape)\n",
    "print (train_con4features.shape)\n",
    "print (train_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
